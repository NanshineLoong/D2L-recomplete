{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import d2l.torch as d2l\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    batch_size, length = X.shape[0], X.shape[1]\n",
    "    X = X.reshape(batch_size, length, num_heads, -1)\n",
    "\n",
    "    X = torch.permute(X, (0, 2, 1, 3))\n",
    "    X = X.reshape((-1, X.shape[2], X.shpe[3]))\n",
    "    return X\n",
    "\n",
    "def transpose_out(X, num_heads):\n",
    "    X = X.reshape((-1, num_heads, X.shape[1], X.shape[2]))\n",
    "    X = torch.permute(X, (0, 2, 1, 3))\n",
    "    X = X.reshape(X.shape[0], X.shape[1], -1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return F.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_hiddens, qkv_dim, num_heads, dropout, \n",
    "                 bias=False, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W_q = nn.Linear(num_hiddens, qkv_dim, bias=bias)\n",
    "        self.W_k = nn.Linear(num_hiddens, qkv_dim, bias=bias)\n",
    "        self.W_v = nn.Linear(num_hiddens, qkv_dim, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        q = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        k = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        v = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0\n",
    "            )\n",
    "\n",
    "        d = q.shape[-1]\n",
    "        scores = torch.bmm(q, torch.transpose(k, 1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        out = torch.bmm(self.dropout(self.attention_weights), v)\n",
    "\n",
    "        out = transpose_out(out, self.num_heads)\n",
    "        return self.W_o(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 4)\n",
    "torch.transpose(x, 1, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addnorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.layernorm(X + self.dropout(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lin1 = nn.Linear(num_inputs, num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(num_hiddens, num_outputs)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.lin2(self.relu(self.lin1(X)))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''Multihead Attention + Addnorm + FFN'''\n",
    "    def __init__(self, num_hiddens, qkv_dim, num_heads, norm_shape, ffn_num_inputs,\n",
    "                 ffn_num_hiddens, dropout, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.attention = MultiheadAttention(embedding_dim=num_hiddens, qkv_dim=qkv_dim, num_heads=num_heads)\n",
    "        self.addnorm1 = Addnorm(norm_shape, dropout)\n",
    "        self.ffn = PositionwiseFFN(ffn_num_inputs, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = Addnorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        '''embedding: batch_size, length, embedding_dim'''\n",
    "        X = self.addnorm1(X, self.attention(X,X,X, valid_lens))\n",
    "        X = self.addnorm2(self.ffn(X), X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    '''block'''\n",
    "    def __init__(self, vocab_size, num_hiddens, num_blocks, max_len=1000,\n",
    "                 key_size=768, query_size=768, value_size=768,\n",
    "                  *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blks.add_module(f'block {i}', Block(\n",
    "                key_size, query_size, value_size, num_hiddens))\n",
    "            \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "    \n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        '''embedding: (batch_size, length_of_sentence, embedding_dim)'''\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens), nn.ReLU(),\n",
    "            nn.LayerNorm(num_hiddens), nn.Linear(num_hiddens, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X, predict_pos):\n",
    "        num_pred_positions = predict_pos.shape[1]\n",
    "        batch_idx = torch.arange(X.shape[0])\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "    \n",
    "        pred = X[batch_idx, predict_pos.reshape(-1)]\n",
    "        return self.mlp(pred)\n",
    "\n",
    "class NSP(nn.Module):\n",
    "    def __init__(self, hid_in_features, num_hiddens, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
    "                                    nn.Tanh(), nn.Linear(num_hiddens, 2))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 直接预测单词(不用softmax？)\n",
    "        return self.mlp(X)\n",
    "\n",
    "\n",
    "class  BertModel(nn.Module):\n",
    "    '''embedding + BertEncoder + MaskLM + NSP'''\n",
    "    def __init__(self, embedding_dim, max_len, vocab, num_blocks, num_heads):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.encoder = BertEncoder(num_blocks=num_blocks, \n",
    "                                   embedding_dim=embedding_dim, \n",
    "                                   qkv_dim=embedding_dim, \n",
    "                                   num_heads=num_heads)\n",
    "        self.masklm = MaskLM(embedding_dim, vocab)\n",
    "        self.nsp = NSP(embedding_dim)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, predict_pos=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if predict_pos is not None:\n",
    "            masked_Y_hat = self.masklm(encoded_X, predict_pos)\n",
    "        else:\n",
    "            masked_Y_hat = None\n",
    "\n",
    "        nsp_Y_hat = self.nsp(encoded_X[:, 0, :])\n",
    "\n",
    "        return encoded_X, masked_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'vocab_size' and 'num_hiddens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m predict_pos \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\n\u001b[0;32m     13\u001b[0m     [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m],\n\u001b[0;32m     14\u001b[0m     [\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]\n\u001b[0;32m     15\u001b[0m ])\n\u001b[0;32m     16\u001b[0m vocab \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m)]\n\u001b[1;32m---> 17\u001b[0m bert \u001b[39m=\u001b[39m BertModel(embedding_dim\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, max_len\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, vocab\u001b[39m=\u001b[39;49mvocab, num_blocks\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, num_heads\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     18\u001b[0m masked_tokens, nsp \u001b[39m=\u001b[39m bert(values, segments, predict_pos)\n\u001b[0;32m     19\u001b[0m masked_tokens\u001b[39m.\u001b[39mshape, nsp\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[147], line 32\u001b[0m, in \u001b[0;36mBertModel.__init__\u001b[1;34m(self, embedding_dim, max_len, vocab, num_blocks, num_heads)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, embedding_dim, max_len, vocab, num_blocks, num_heads):\n\u001b[0;32m     31\u001b[0m     \u001b[39msuper\u001b[39m(BertModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m BertEncoder(num_blocks\u001b[39m=\u001b[39;49mnum_blocks, \n\u001b[0;32m     33\u001b[0m                                embedding_dim\u001b[39m=\u001b[39;49membedding_dim, \n\u001b[0;32m     34\u001b[0m                                qkv_dim\u001b[39m=\u001b[39;49membedding_dim, \n\u001b[0;32m     35\u001b[0m                                num_heads\u001b[39m=\u001b[39;49mnum_heads)\n\u001b[0;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasklm \u001b[39m=\u001b[39m MaskLM(embedding_dim, vocab)\n\u001b[0;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsp \u001b[39m=\u001b[39m NSP(embedding_dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'vocab_size' and 'num_hiddens'"
     ]
    }
   ],
   "source": [
    "#---------------------- test ------------------#\n",
    "# MaskLM\n",
    "# x = torch.arange(32).reshape(2, 4, 4).float()\n",
    "# pos = torch.arange(4).reshape(2, 2)\n",
    "# vocab = [i for i in range(10)]\n",
    "# masklm = MaskLM(4, vocab)\n",
    "# masklm(x, pos).shape\n",
    "\n",
    "# BertModel\n",
    "values = torch.randint(0, 3, (2, 4))\n",
    "segments = torch.cat((torch.zeros(2, 2), torch.ones(2, 2)), dim=1).int()\n",
    "predict_pos = torch.tensor([\n",
    "    [1, 2],\n",
    "    [2, 3]\n",
    "])\n",
    "vocab = [i for i in range(10)]\n",
    "bert = BertModel(embedding_dim=4, max_len=4, vocab=vocab, num_blocks=2, num_heads=2)\n",
    "masked_tokens, nsp = bert(values, segments, predict_pos)\n",
    "masked_tokens.shape, nsp.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corpus(tokens):\n",
    "    '''Count token frequencies'''\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens \n",
    "                  for token in line]\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    '''Vocabulary for text'''\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        \n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token : i for \n",
    "                             i, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "        \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id_to_token)\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.to_tokens(index) for index in indices]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(sentence1, sentence2=None):\n",
    "    sentence = ['<cls>'] + sentence1 + ['<seg>']\n",
    "    segments = [0] * len(sentence1)\n",
    "\n",
    "    if sentence2 is not None:\n",
    "        sentence += sentence2 + ['<seg>']\n",
    "        segments += [1] * (len(sentence2) + 1)\n",
    "    return sentence, segments\n",
    "\n",
    "def _get_nsp_data(paragraphs, max_len):\n",
    "    examples = []\n",
    "    is_next = True\n",
    "    for paragraph in paragraphs:\n",
    "        for i in range(len(paragraph) - 1):\n",
    "            sentence1 = paragraph[i]\n",
    "            if random.random() < 0.5:\n",
    "                sentence2 = paragraph[i + 1]\n",
    "                is_next = True\n",
    "            else:\n",
    "                sentence2 = random.choice(random.choice(paragraphs))\n",
    "                is_next = False\n",
    "            \n",
    "            if len(sentence1) + len(sentence2) + 3 > max_len:\n",
    "                continue\n",
    "\n",
    "            sentence, segments = get_tokens_and_segments(sentence1, sentence2)\n",
    "            examples.append((sentence, segments, is_next))\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pos = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in ('<cls>', '<sep>'):\n",
    "            candidate_pos.append(i)\n",
    "    \n",
    "    predict = []\n",
    "    random.shuffle(candidate_pos)\n",
    "    mask_token = None\n",
    "    for i in range(max(1, round(0.15 * (len(tokens) - 3)))):\n",
    "        if random.random() < 0.8:\n",
    "            mask_token = '<mask>'\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                mask_token = tokens[candidate_pos[i]]\n",
    "            else:\n",
    "                mask_token = random.choice(vocab.idx_to_token)\n",
    "        predict.append((candidate_pos[i], tokens[candidate_pos[i]]))\n",
    "        tokens[candidate_pos[i]] = mask_token\n",
    "    \n",
    "    predict = sorted(predict, key=lambda x: x[0])\n",
    "\n",
    "    predict_pos = [v[0] for v in predict]\n",
    "    predict_label = [v[1] for v in predict]\n",
    "\n",
    "    return vocab[tokens], predict_pos, vocab[predict_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _padding_inputs(examples, max_len, vocab):\n",
    "    max_masked_len = round(max_len * 0.15)\n",
    "    all_tokens, all_segments, all_weights, all_pred_pos, all_pred_labels, all_is_next = [], [], [], [], [], []\n",
    "    valid_lens = []\n",
    "    for tokens, predict_pos, predict_label, segments, is_next in examples:\n",
    "        all_tokens.append(torch.tensor(\n",
    "            tokens + [vocab['<pad>']] * (max_len - len(tokens)), dtype=torch.long))\n",
    "        all_segments.append(torch.tensor(\n",
    "            segments + [0] * (max_len - len(segments)), dtype=torch.long\n",
    "        ))\n",
    "        valid_lens.append(torch.tensor(len(tokens), dtype=torch.float))\n",
    "\n",
    "        all_pred_pos.append(torch.tensor(\n",
    "            predict_pos + [0] * (max_masked_len - len(predict_pos)), dtype=torch.long\n",
    "        ))\n",
    "        all_pred_labels.append(torch.tensor(\n",
    "            predict_label + [0] * (max_masked_len - len(predict_label)), dtype=torch.long\n",
    "        ))\n",
    "        all_weights.append(torch.tensor(\n",
    "            [1.0] *  len(predict_label) + [0.0] * (max_masked_len - len(predict_label)), dtype=torch.float\n",
    "        ))\n",
    "        all_is_next.append(torch.tensor(\n",
    "            is_next, dtype=torch.long\n",
    "        ))\n",
    "    \n",
    "    return (all_tokens, all_segments, valid_lens, all_pred_pos, all_pred_labels,\n",
    "            all_weights, all_is_next)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WikiDataset(Dataset):\n",
    "    def __init__(self, data_path, max_len):\n",
    "        with open(data_path, 'r') as f:\n",
    "            paragraphs = f.readlines()\n",
    "        paragraphs = [paragraph.strip().lower().split('.')\n",
    "                    for paragraph in paragraphs if len(paragraph.split('.') >= 2)]\n",
    "        random.shuffle(paragraphs)\n",
    "\n",
    "        paragraphs = [[sentence.split() for sentence in paragraph] \n",
    "                      for paragraph in paragraphs]  # 三维列表\n",
    "        sentences = [sentence for paragraph in paragraphs \n",
    "                     for sentence in paragraph] # 转为二维列表\n",
    "        self.vocab = Vocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "\n",
    "        # 获取下一句子预测任务的数据\n",
    "        examples = _get_nsp_data(paragraphs=paragraphs, max_len=max_len)\n",
    "\n",
    "        examples = [_get_mlm_data_from_tokens(tokens, self.vocab) \n",
    "                    + (segments, is_next)\n",
    "                    for tokens, segments, is_next in examples]\n",
    "        \n",
    "        # padding bert\n",
    "        (self.all_tokens, self.all_segments, self.valid_lens, self.all_pred_pos, self.all_pred_labels,\n",
    "            self.all_weights, self.all_is_next) = _padding_inputs(examples, max_len, self.vocab)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_tokens[idx], self.all_segments[idx], \n",
    "                self.valid_lens[idx], self.all_pred_pos[idx], \n",
    "                self.all_pred_labels[idx], self.all_weights[idx],\n",
    "                self.all_is_next[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_is_next)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wiki(data_path, batch_size, max_len):\n",
    "    \"\"\"加载wikiText-2数据\"\"\"\n",
    "    train_set = _WikiDataset(data_path=data_path, max_len=max_len)\n",
    "    train_iter = DataLoader(train_set, batch_size, shuffle=True )\n",
    "    return train_iter, train_set.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2709165333.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[156], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def _get_batch_loss()\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def _get_batch_loss(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # 前向传播\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
    "                                  valid_lens_x.reshape(-1),\n",
    "                                  pred_positions_X)\n",
    "    # 计算遮蔽语言模型损失\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # 计算下一句子预测任务的损失\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y).sum()\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, devices, max_step):\n",
    "    net = net.to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    num_steps_reached = False\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss', xlim=[1, max_step],\n",
    "                            legend=['mlm', 'nsp'])\n",
    "    metric = d2l.Accumulator(4)\n",
    "\n",
    "    while(step < max_step and not num_steps_reached):\n",
    "        for tokens, segments, valid_lens, pred_pos, \\\n",
    "            weights, pred_labels, is_next in train_iter:\n",
    "            tokens = tokens.to(devices[0])\n",
    "            segments = segments.to(devices[0])\n",
    "            valid_lens = valid_lens.to(devices[0])\n",
    "            pred_pos = pred_pos.to(devices[0])\n",
    "            weights = weights.to(devices[0])\n",
    "            pred_labels = pred_labels.to(devices[0])\n",
    "            is_next = is_next.to(devices[0])\n",
    "\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "\n",
    "            mlm_l, nsp_l, l = _get_batch_loss(\n",
    "                net, loss, \n",
    "            )\n",
    "\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "\n",
    "            metric.add(mlm_l, nsp_l, tokens.shape[0], 1)\n",
    "            timer.stop()\n",
    "            animator.add(step + 1,\n",
    "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            \n",
    "            step += 1\n",
    "            if step == max_step:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "    \n",
    "    print(f'MLM loss {metric[0] / metric[3]: .3f}, NSP loss {metric[1] / metric[3]: .3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on {str(devices)}')\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_gpus():\n",
    "    '''Return all available GPUs, or [cpu(), ], if no GPU exists'''\n",
    "    devices = [torch.device(f\"cuda:{i}\")\n",
    "               for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "data_path = os.path.join(\"../data/wikitext-2\", \"wiki.train.tokens\")\n",
    "train_iter, vocab = load_data_wiki(data_path, batch_size, max_len)\n",
    "\n",
    "net = BertModel(embedding_dim=4, max_len=4, vocab=vocab, num_blocks=2, num_heads=2)\n",
    "devices = try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "train_bert(net, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
